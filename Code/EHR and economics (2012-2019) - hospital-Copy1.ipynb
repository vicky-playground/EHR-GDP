{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d293196b",
   "metadata": {},
   "source": [
    "# Clean and display the three datasets (EHR, GDP, Income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6768141",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GDP_CA_2001_2020.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29364\\4036795574.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# load GDP dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mdf_GDP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GDP_CA_2001_2020.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mdf_GDP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GDP_CA_2001_2020.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# load GDP dataset\n",
    "df_GDP = pd.read_csv('GDP_CA_2001_2020.csv')\n",
    "df_GDP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb88140",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_GDP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd18d2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_GDP.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f544b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the noisy data\n",
    "df_GDP['GeoName'] = df_GDP['GeoName'].str.rstrip(', CA')\n",
    "df_GDP.drop(df_GDP.tail(4).index,inplace=True) # drop last n rows\n",
    "df_GDP = df_GDP.drop(['GeoFIPS','Region','TableName','LineCode','IndustryClassification'],axis=1)\n",
    "df_GDP = df_GDP[df_GDP.Description == 'Current-dollar GDP (thousands of current dollars)']\n",
    "df_GDP = df_GDP.drop(df_GDP.columns[3:14],axis=1)\n",
    "df_GDP = df_GDP.dropna()\n",
    "df_GDP = df_GDP.drop(df_GDP.columns[-1:],axis=1)\n",
    "df_GDP = df_GDP.drop(df_GDP.columns[1:2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563c3aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_GDP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffad52",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load EHR dataset\n",
    "df_EHR = pd.read_csv('EHR_Incentive_Program_Payments_Hospitals.csv')\n",
    "df_EHR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c5c2f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EHR.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88c235",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EHR.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f139612",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the noisy data\n",
    "df_EHR = df_EHR.drop(df_EHR.columns[0:7], axis=1)\n",
    "df_EHR = df_EHR.drop(df_EHR.columns[[0,2,3,4,5,6,8,9,10,11,12]], axis=1)\n",
    "#df_EHR = df_EHR.drop(['Payment__1','Payment_Cr','Payee_Name','Payee_NPI','total_rece','Latitude','Longitude','Program_Ye'],axis=1)\n",
    "df_EHR = df_EHR.dropna()\n",
    "df_EHR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619bf93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EHR = df_EHR.rename({'Business_County':'County','Last_Payment_Year': 'Payment_Ye', 'total_payments': 'EHR Total Payment'}, axis=1) \n",
    "df_EHR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c0f55",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EHR['Payment_Ye'] = df_EHR['Payment_Ye'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a527c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load median income dataset\n",
    "df_income = pd.read_excel('B-6__Comparison_By_County.xlsx')\n",
    "df_income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d87ef",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_income.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c2b6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_income.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39259f3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean the noisy data\n",
    "df_income = df_income[(df_income[\"Taxable Year\"]>=2012) & (df_income[\"Taxable Year\"]<=2019)]\n",
    "df_income[\"Taxable Year\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afa28d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_income['Taxable Year'] = df_income['Taxable Year'].apply(lambda x: str(x))\n",
    "df_income = df_income.drop(df_income.columns[[3,4,6,7,8,9,10,11,12,13]], axis=1)\n",
    "df_income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436368e3",
   "metadata": {},
   "source": [
    "# Aggregate the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d6a58",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_GDP = df_GDP.groupby(['GeoName','Unit']).sum().reset_index()\n",
    "df_GDP.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff841d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_EHR = df_EHR.groupby(['County','Payment_Ye']).sum().reset_index()\n",
    "df_EHR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8527159",
   "metadata": {},
   "source": [
    "# Reshape the dataset (GDP) from wide format to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4ffc3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_list = df_GDP.columns[3:]\n",
    "df_GDP = pd.melt(df_GDP, id_vars=['GeoName','Unit'], value_vars=year_list) #Pandas.melt() unpivots a DataFrame from wide format to long format\n",
    "df_GDP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082ffe4",
   "metadata": {},
   "source": [
    "# Change the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4ec80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_GDP = df_GDP.rename({'GeoName':'County','Unit': 'Unit of GDP', 'variable': 'Year', 'value':'GDP'}, axis=1) \n",
    "df_GDP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a2c90",
   "metadata": {},
   "source": [
    "# Combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33272f18",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df_GDP.merge(df_EHR, how='inner', left_on=['County', 'Year'], right_on=['County', 'Payment_Ye'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac776c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(df_income, how='inner', left_on=['County', 'Year'], right_on=['County', 'Taxable Year'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91040dbf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[4,6]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f0807",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# swap the columns\n",
    "cols = list(df.columns)\n",
    "a, b = cols.index('Unit of GDP'), cols.index('Year')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70b36d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb21c62",
   "metadata": {},
   "source": [
    "# Year-by-year EHR spending correlation with GDP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fab4ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.regplot(x='GDP', y='EHR Total Payment', data=df, ci=None, scatter_kws={'s':100, 'facecolor':'red'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733bb65",
   "metadata": {},
   "source": [
    "## Outlier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594f556",
   "metadata": {},
   "source": [
    "Quantile-based Flooring and Capping\n",
    "In this technique, we will do the flooring (e.g., the 10th percentile) for the lower values and capping (e.g., the 90th percentile) for the higher values. The lines of code below print the 10th and 90th percentiles of the variable 'Income', respectively. These values will be used for quantile-based flooring and capping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4e394",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df['GDP'].quantile(0.10))\n",
    "print(df['GDP'].quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f300a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df['GDP'].skew())\n",
    "\n",
    "# remove the outlier\n",
    "df[\"GDP\"] = np.where(df[\"GDP\"] <1343309.7, 1343309.7,df['GDP'])\n",
    "df[\"GDP\"] = np.where(df[\"GDP\"] >245565614.8, 245565614.8,df['GDP'])\n",
    "\n",
    "print(df['GDP'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9615b0b",
   "metadata": {},
   "source": [
    "The above output shows that the skewness value came down from 2.68 to 1.21, confirming that the distribution has been treated for extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf2c9d5",
   "metadata": {},
   "source": [
    "## 1. Application of Linear Regression - EHR & GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffdb90d",
   "metadata": {},
   "source": [
    "Linear regression is suited for estimating continuous values\n",
    "\n",
    "Linear regression model fit line: The plot shows us how well we are able to fit the relationship between the GDP value and the total payment of EHR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9febab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Build the model\n",
    "# ================================================\n",
    "\n",
    "# Training data\n",
    "X = df.loc[:,[\"GDP\"]]  # features matrix\n",
    "y = df.loc[:,'EHR Total Payment']  # target (response) matix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d31b33",
   "metadata": {},
   "source": [
    "### Predicting test set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb5800",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Comparing the test values and the predicted values\n",
    "comparison_df = pd.DataFrame({\"Actual\":y_test,\"Predicted\":y_pred})\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa8de6",
   "metadata": {},
   "source": [
    "### Checking the residuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce6666",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa77ef",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "pd.DataFrame({'True Values(y test)': y_test, 'Predicted Values': y_pred}).hvplot.scatter(x='True Values(y test)', y='Predicted Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbd7fe",
   "metadata": {},
   "source": [
    "The values seem to align linearly, which shows that the model is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074a868",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c8f37e",
   "metadata": {},
   "source": [
    "Check the coefficients, P values, MAE, MSE, RMSE, R2 square\n",
    "\n",
    "Coefficients: Quantify the strength of relationship with correaltion(R)\n",
    "\n",
    "P value: The probability that randomly drawn points will result in the similarly strong relationship, so the smaller the p-value, the more confidence we have in the predictions we make with the line.\n",
    "\n",
    "MAE is the easiest to understand, because it's the average error.\n",
    "\n",
    "MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "\n",
    "RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are loss functions, because we want to minimize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82de1b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val(model):\n",
    "    pred = cross_val_score(model, X, y, cv=10)\n",
    "    return pred.mean()\n",
    "\n",
    "def print_evaluate(true, predicted):  \n",
    "    mae = metrics.mean_absolute_error(true, predicted)\n",
    "    mse = metrics.mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
    "    r2_square = metrics.r2_score(true, predicted)\n",
    "    print('MAE:', mae)\n",
    "    print('MSE:', mse)\n",
    "    print('RMSE:', rmse)\n",
    "    print('R2 Square', r2_square)\n",
    "    print('__________________________________')\n",
    "    \n",
    "def evaluate(true, predicted):\n",
    "    mae = metrics.mean_absolute_error(true, predicted)\n",
    "    mse = metrics.mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n",
    "    r2_square = metrics.r2_score(true, predicted)\n",
    "    return mae, mse, rmse, r2_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a64823",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coefficient\n",
    "coeff_df = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f899b",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "Holding all other features fixed, a 1 unit (thousands of dollars) increase in GDP is associated with an increase of $0.034668 in EHR payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a03d2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ce98c",
   "metadata": {},
   "source": [
    "## Check the distribution of the error terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a52fe",
   "metadata": {},
   "source": [
    "In linear regression we assume that the error term follows normal distribution. So we have to check this assumption before we can use the model for making predictions. We check this by looking at the histogram of the error term visually, making sure that the error terms are normally distributed around zero and that the left and right side are broadly similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e009215",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Residual Histogram\n",
    "pd.DataFrame({'Error Values': (y_test - y_pred)}).hvplot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9af25",
   "metadata": {},
   "source": [
    "## Comparing machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97112636",
   "metadata": {},
   "source": [
    "### Application of Decision Tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdba186",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "model = DecisionTreeRegressor(random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "#Predicting using test set \n",
    "y_pred = model.predict(X_test)\n",
    "mae=metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse=metrics.mean_squared_error(y_test, y_pred)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf39c302",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Decision Tree regression\", *evaluate(y_test, test_pred) , cross_val(DecisionTreeRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66d7ff",
   "metadata": {},
   "source": [
    "### Application of Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb106a6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\n",
    "model.fit(X_train,y_train)\n",
    "#Predicting the SalePrices using test set \n",
    "y_pred = model.predict(X_test)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8fb6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Random Forest Regression\", *evaluate(y_test, test_pred) , cross_val(RandomForestRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6839c",
   "metadata": {},
   "source": [
    "### Application of Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d085ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model= SVR(kernel='rbf')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred_svm=model.predict(X_test)\n",
    "#y_pred_svm = cross_val_predict(regressor, x, y)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d8a6b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Support Vector Regression\", *evaluate(y_test, test_pred) , cross_val(SVR())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1824465",
   "metadata": {},
   "source": [
    "### Random Sample Consensus(RANSAC) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e92cfcd",
   "metadata": {},
   "source": [
    "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates. Therefore, it also can be interpreted as an outlier detection method.\n",
    "\n",
    "A basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, for example, from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb45a08",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "#train_pred = model.predict(X_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcb339",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Random Sample Consensus\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a4171",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7cb3a",
   "metadata": {},
   "source": [
    "Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Sescent is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regards to the parameters vector, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09130a54",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\n",
    "sgd_reg.fit(X_train, y_train)\n",
    "\n",
    "test_pred = sgd_reg.predict(X_test)\n",
    "train_pred = sgd_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5fb61",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred),cross_val(SGDRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d06174",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaf892",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435ca4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df.set_index('Model', inplace=True)\n",
    "results_df['MAE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e63c8",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8825f9b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29364\\1257396012.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MSE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'barh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "results_df['MSE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8a0a1",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b9a0b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df['RMSE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba12e7e",
   "metadata": {},
   "source": [
    "### R2 Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff605e07",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1cde91",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacd8a0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee33d816",
   "metadata": {},
   "source": [
    "# 2. Per-capita EHR spending correlation with median income in 2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4149df8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the per capita payment from EHR \n",
    "df[\"EHR Per Capita\"] = (df[\"EHR Total Payment\"]/df[\"Population\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb0664",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2 = df[df[\"Year\"]==\"2019\"]\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e89225",
   "metadata": {},
   "source": [
    "## Outlier Treatment: Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69d2aa",
   "metadata": {},
   "source": [
    "Transformation of the skewed variables may also help correct the distribution of the variables. These could be logarithmic, square root, or square transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559390b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29364\\2494199717.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Median Income'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Median Income'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# shrink the difference among data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_2' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "df_2['Median Income'] = df_2['Median Income'].apply(lambda x: math.log(x)) # shrink the difference among data\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0fe2ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29364\\3888880902.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'EHR Per Capita'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_2' is not defined"
     ]
    }
   ],
   "source": [
    "df_2['EHR Per Capita'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40a7ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df['Median Income'].skew())\n",
    "print(df_2['Median Income'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77741a78",
   "metadata": {},
   "source": [
    "The above output shows that the skewness value came down from 1.38 to 0.83, confirming that the distribution has been treated for extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d33988",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='Median Income', y='EHR Per Capita', data=df_2, ci=None, scatter_kws={'s':100, 'facecolor':'red'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab74649",
   "metadata": {},
   "source": [
    "## Application of Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b16826",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Build the model\n",
    "# ================================================\n",
    "\n",
    "# Training data\n",
    "X = df_2.loc[:,[\"Median Income\"]]  # features matrix\n",
    "y = df_2.loc[:,'EHR Per Capita']  # target (response) matix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e51ae",
   "metadata": {},
   "source": [
    "### Predicting test set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d03b3a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Comparing the test values and the predicted values\n",
    "comparison_df = pd.DataFrame({\"Actual\":y_test,\"Predicted\":y_pred})\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c72da5",
   "metadata": {},
   "source": [
    "### Checking the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979caca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "residuals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05556e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'True Values(y test)': y_test, 'Predicted Values': y_pred}).hvplot.scatter(x='True Values(y test)', y='Predicted Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46bd97",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f32e93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coefficient\n",
    "coeff_df = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79604a9",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "Holding all other features fixed, a 1 unit increase in log of Median Income is associated with a decrease of $6.177453 in total payment per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263ad33",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred) , cross_val(LinearRegression())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9ea41",
   "metadata": {},
   "source": [
    "## Check the distribution of the error terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a03860",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Residual Histogram\n",
    "pd.DataFrame({'Error Values': (y_test - y_pred)}).hvplot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9eb3c4",
   "metadata": {},
   "source": [
    "## Comparing machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99157d0",
   "metadata": {},
   "source": [
    "### Application of Decision Tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839633b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "model = DecisionTreeRegressor(random_state = 0)\n",
    "model.fit(X_train, y_train)\n",
    "#Predicting using test set \n",
    "y_pred = model.predict(X_test)\n",
    "mae=metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse=metrics.mean_squared_error(y_test, y_pred)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c5df",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Decision Tree regression\", *evaluate(y_test, test_pred) , cross_val(DecisionTreeRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bde99",
   "metadata": {},
   "source": [
    "### Application of Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d010b79",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\n",
    "model.fit(X_train,y_train)\n",
    "#Predicting the SalePrices using test set \n",
    "y_pred = model.predict(X_test)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a9ff4b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Random Forest Regression\", *evaluate(y_test, test_pred) , cross_val(RandomForestRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c414b5",
   "metadata": {},
   "source": [
    "### Application of Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ebd9b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "model= SVR(kernel='rbf')\n",
    "model.fit(X_train,y_train)\n",
    "y_pred_svm=model.predict(X_test)\n",
    "#y_pred_svm = cross_val_predict(regressor, x, y)\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a401c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Support Vector Regression\", *evaluate(y_test, test_pred) , cross_val(SVR())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7b26c",
   "metadata": {},
   "source": [
    "### Random Sample Consensus(RANSAC) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1f6ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "#train_pred = model.predict(X_train)\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9151c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Random Sample Consensus\", *evaluate(y_test, test_pred) , cross_val(RANSACRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7621a",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dceb7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\n",
    "sgd_reg.fit(X_train, y_train)\n",
    "\n",
    "test_pred = sgd_reg.predict(X_test)\n",
    "train_pred = sgd_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8de9c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "train_pred = model.predict(X_train)\n",
    "\n",
    "print('Test set evaluation:\\n_____________________________________')\n",
    "print_evaluate(y_test, test_pred)\n",
    "\n",
    "results_df_2 = pd.DataFrame(data=[[\"Stochastic Gradient Descent\", *evaluate(y_test, test_pred),cross_val(SGDRegressor())]], \n",
    "                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square', \"Cross Validation\"])\n",
    "results_df = results_df.append(results_df_2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b830a3",
   "metadata": {},
   "source": [
    "## Models Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8036f6",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f486d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df.set_index('Model', inplace=True)\n",
    "results_df['MAE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2d0f4",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380aea9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df['MSE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f6d97",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad19e66",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df['RMSE'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d147d",
   "metadata": {},
   "source": [
    "### R2 Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb902a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df['R2 Square'].plot(kind='barh', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719d5c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717b999a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
